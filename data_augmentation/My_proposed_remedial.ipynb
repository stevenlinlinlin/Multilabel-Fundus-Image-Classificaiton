{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import copy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(csv_path, image_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    images = []\n",
    "    # possible_extensions = ['.tif', '.png']\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # image_path = None\n",
    "        # for ext in possible_extensions:\n",
    "        #     temp_path = os.path.join(image_dir, f\"{row['ID']}{ext}\")\n",
    "        #     if os.path.exists(temp_path):\n",
    "        #         image_path = temp_path\n",
    "        #         break\n",
    "        # if image_path is None:\n",
    "        #     raise FileNotFoundError(f\"No image found for ID {row['ID']} with extensions {possible_extensions}\")\n",
    "        \n",
    "        # image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # image = image.resize((384,384))  # resize the image to 384x384\n",
    "        \n",
    "        # image = np.array(image)\n",
    "        labels = row.drop('ID').to_numpy()\n",
    "        # images.append({'features': image, 'labels': labels, 'id': row['ID']})\n",
    "        images.append({'labels': labels, 'id': row['ID']})\n",
    "    \n",
    "    return images, df.columns[1:]\n",
    "\n",
    "\n",
    "csv_path = '../data/fundus/MuReD/train_data.csv'\n",
    "image_dir = '../data/fundus/MuReD/images/images/'\n",
    "dataset, labels = load_dataset(csv_path, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_sample(sample, new_id):\n",
    "    clone = {'features': sample['features'].copy(), 'labels': sample['labels'].copy(), 'id': new_id}\n",
    "    return clone\n",
    "\n",
    "def calculate_ir_lbl(D, label_index, label_counts):\n",
    "    label_count = label_counts[label_index]\n",
    "    max_label_count = np.max(label_counts)\n",
    "    return max_label_count / label_count if label_count != 0 else float('inf')\n",
    "\n",
    "def calculate_scu_mble_ins(instance, ir_lbl):\n",
    "    instance_labels = np.where(instance['labels'] == 1)[0]\n",
    "    k = len(instance_labels)\n",
    "    if k == 0:\n",
    "        return 0\n",
    "    product_ir_lbl = reduce(lambda x, y: x * y, (ir_lbl[label] for label in instance_labels), 1)\n",
    "    mean_ir_lbl = np.mean([ir_lbl[label] for label in instance_labels]) if instance_labels.size > 0 else 1\n",
    "    scumble_ins = 1 - (1 / mean_ir_lbl) * (product_ir_lbl ** (1 / k))\n",
    "    return scumble_ins\n",
    "\n",
    "def calculate_scu_mble(D, ir_lbl):\n",
    "    scumble_ins = [calculate_scu_mble_ins(instance, ir_lbl) for instance in D]\n",
    "    scumble = np.mean(scumble_ins)\n",
    "    return scumble, scumble_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE\n",
    "# def nearest_neighbour(X):\n",
    "#     nbs=NearestNeighbors(n_neighbors=3,metric='euclidean',algorithm='kd_tree').fit(X)\n",
    "#     _,indices= nbs.kneighbors(X)\n",
    "#     return indices\n",
    "\n",
    "# def smote(ref, nn_indices, imbalance_images):\n",
    "#     neighbour = random.choice(nn_indices[ref,1:])\n",
    "#     ratio = random.random()\n",
    "#     gap = imbalance_images[ref,:] - imbalance_images[neighbour,:]\n",
    "#     new_x = np.array(imbalance_images[ref,:] + ratio * gap)\n",
    "#     return new_x.reshape(384,384,3).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_entropy(probabilities):\n",
    "#     return -np.sum([p * np.log2(p) if p > 0 else 0 for p in probabilities])\n",
    "\n",
    "# def calculate_gini(probabilities):\n",
    "#     return 1 - np.sum([p**2 for p in probabilities])\n",
    "\n",
    "# KMeans finding minority class\n",
    "def get_minority_class_index(class_counts):\n",
    "    data = np.array(class_counts)\n",
    "    data = data.reshape(-1, 1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data_normalized = scaler.fit_transform(data)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    clusters = kmeans.fit_predict(data_normalized)\n",
    "\n",
    "    cluster_sums = {}\n",
    "    for cluster in np.unique(clusters):\n",
    "        cluster_sums[cluster] = np.mean(data[clusters == cluster])\n",
    "\n",
    "    min_sum_cluster = min(cluster_sums, key=cluster_sums.get)\n",
    "    minority_class_index = np.where(clusters == min_sum_cluster)[0]\n",
    "        \n",
    "    return minority_class_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label counts: [396 395 135 211 125 126 130 71 63 50 44 48 47 46 37 29 28 26 24 209]\n",
      "minority class index: [ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "ir_lbl: [1.0, 1.0025316455696203, 2.933333333333333, 1.8767772511848342, 3.168, 3.142857142857143, 3.046153846153846, 5.577464788732394, 6.285714285714286, 7.92, 9.0, 8.25, 8.425531914893616, 8.608695652173912, 10.702702702702704, 13.655172413793103, 14.142857142857142, 15.23076923076923, 16.5, 1.894736842105263], ir_mean: 7.1181649096420205\n",
      "scumble: 0.028608447515591443\n",
      "imbalance index counts: 293\n",
      "added 1588 samples\n",
      "Counter({3: 433, 5: 345, 19: 321, 4: 303, 11: 228, 7: 219, 14: 195, 9: 179, 6: 126, 2: 123, 8: 103, 17: 99, 16: 83, 18: 44, 10: 32, 13: 28, 15: 16, 12: 11})\n"
     ]
    }
   ],
   "source": [
    "def my_remedial(ori_D, P, threshold=0.5):\n",
    "    D = copy.deepcopy(ori_D)\n",
    "    samples_to_clone = int(len(D) * P)\n",
    "    \n",
    "    # Calculate label counts\n",
    "    label_counts = np.sum([instance['labels'] for instance in D], axis=0)\n",
    "    print(f\"label counts: {label_counts}\")\n",
    "    # Get minority class index with KMeans\n",
    "    minority_class_index = get_minority_class_index(label_counts)\n",
    "    print(f\"minority class index: {minority_class_index}\")\n",
    "    \n",
    "    # Calculate imbalance levels\n",
    "    ir_lbl = [calculate_ir_lbl(D, i, label_counts) for i in range(len(labels))]\n",
    "    ir_mean = np.mean(ir_lbl)\n",
    "    print(f\"ir_lbl: {ir_lbl}, ir_mean: {ir_mean}\")\n",
    "    # Calculate SCUMBLE\n",
    "    scumble, scumble_ins = calculate_scu_mble(D, ir_lbl)\n",
    "    print(f\"scumble: {scumble}\")\n",
    "    \n",
    "    imbalance_index = []\n",
    "    # imbalance_images = []\n",
    "    for i in range(len(D)):\n",
    "        if scumble_ins[i] > scumble:\n",
    "            imbalance_index.append(i)\n",
    "            # imbalance_images.append(D[i]['features'].flatten())\n",
    "    \n",
    "    # imbalance_images = np.array(imbalance_images)\n",
    "    # indices = nearest_neighbour(imbalance_images)\n",
    "    \n",
    "    print(f\"imbalance index counts: {len(imbalance_index)}\")\n",
    "    new_id_counter = 0\n",
    "    new_instances = []\n",
    "    new_labels_counter = Counter()\n",
    "    while samples_to_clone > 0:\n",
    "        samples = random.sample(imbalance_index, min(samples_to_clone, len(imbalance_index)))\n",
    "        for sample_index in samples:\n",
    "            # smote\n",
    "            # ref = imbalance_index.index(sample_index)\n",
    "            # new_x = smote(ref, indices, imbalance_images)\n",
    "            \n",
    "            instance = D[sample_index]\n",
    "            clone_instance = copy.deepcopy(instance)\n",
    "            # clone_instance['id'] = f\"DA_{new_id_counter}\"\n",
    "            # clone_instance['features'] = new_x\n",
    "            \n",
    "            # Add minority labels\n",
    "            # clone_instance['labels'][[label for label in range(len(labels)) if ir_lbl[label] <= ir_mean]] = 0\n",
    "            clone_instance['labels'][[label for label in range(len(labels)) if label not in minority_class_index]] = 0\n",
    "            if np.sum(clone_instance['labels']) == 0:\n",
    "                continue\n",
    "            \n",
    "            new_instances.append(clone_instance)\n",
    "            new_labels_counter.update(np.where(clone_instance['labels'] == 1)[0])\n",
    "            new_id_counter += 1\n",
    "            samples_to_clone -= 1\n",
    "            if samples_to_clone <= 0:\n",
    "                break\n",
    "            \n",
    "    D.extend(new_instances)\n",
    "    print(f\"added {new_id_counter+1} samples\")\n",
    "    print(new_labels_counter)\n",
    "    return D\n",
    "\n",
    "\n",
    "preprocessed_dataset = my_remedial(dataset, P=0.9)#, threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID        aria_c_25_1aria_c_7_2aria_c_38_2aria_c_2_8aria...\n",
      "DR                                                      396\n",
      "NORMAL                                                  395\n",
      "MH                                                      258\n",
      "ODC                                                     644\n",
      "TSLN                                                    428\n",
      "ARMD                                                    471\n",
      "DN                                                      256\n",
      "MYA                                                     290\n",
      "BRVO                                                    166\n",
      "ODP                                                     229\n",
      "CRVO                                                     76\n",
      "CNV                                                     276\n",
      "RS                                                       58\n",
      "ODE                                                      74\n",
      "LS                                                      232\n",
      "CSR                                                      45\n",
      "HTR                                                     111\n",
      "ASR                                                     125\n",
      "CRS                                                      68\n",
      "OTHER                                                   530\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def save_preprocessed_dataset(D, labels, image_dir, output_csv_path):\n",
    "    # if not os.path.exists(image_dir):\n",
    "    #     os.makedirs(image_dir)\n",
    "\n",
    "    label_data = []\n",
    "    image_names = []\n",
    "    for instance in D:\n",
    "        # if 'DA_' in instance['id']:\n",
    "        #     image_path = os.path.join(image_dir, f\"{instance['id']}.png\")\n",
    "        #     image = Image.fromarray(instance['features'])\n",
    "        #     image.save(image_path)\n",
    "        # else:\n",
    "        #     possible_extensions = ['.tif', '.png']\n",
    "        #     for ext in possible_extensions:\n",
    "        #         temp_path = os.path.join(image_dir, f\"{instance['id']}{ext}\")\n",
    "        #         if os.path.exists(temp_path):\n",
    "        #             image_path = temp_path\n",
    "        #             break\n",
    "            \n",
    "        label_data.append(instance['labels'])\n",
    "        image_names.append(instance['id'])\n",
    "\n",
    "    label_df = pd.DataFrame(label_data, columns=labels)\n",
    "    label_df.insert(0, 'ID', image_names)\n",
    "    label_df.to_csv(os.path.join(output_csv_path), index=False)\n",
    "    \n",
    "    counts = label_df.sum(axis=0)\n",
    "    counts.to_dict()\n",
    "    print(counts)\n",
    "\n",
    "\n",
    "output_dir = '../data/fundus/MuReD/images/my_remedial_smote384/'\n",
    "output_csv_path = '../data/fundus/MuReD/myproposed_kmean2_90_train_data.csv'\n",
    "save_preprocessed_dataset(preprocessed_dataset, labels, output_dir, output_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilabel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
