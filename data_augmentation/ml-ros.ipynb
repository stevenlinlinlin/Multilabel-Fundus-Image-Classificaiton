{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset size: 2057\n",
      "added 162 samples\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(csv_path, image_dir, remedial_image_dir=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    images = []\n",
    "    possible_extensions = ['.tif', '.png']\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        image_path = None\n",
    "        if row['ID'].startswith(\"DA_\") and remedial_image_dir is not None:\n",
    "            image_path = os.path.join(remedial_image_dir, f\"{row['ID']}.png\")\n",
    "        else:\n",
    "            for ext in possible_extensions:\n",
    "                temp_path = os.path.join(image_dir, f\"{row['ID']}{ext}\")\n",
    "                if os.path.exists(temp_path):\n",
    "                    image_path = temp_path\n",
    "                    break\n",
    "            if image_path is None:\n",
    "                raise FileNotFoundError(f\"No image found for ID {row['ID']} with extensions {possible_extensions}\")\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        labels = row.drop('ID').to_numpy()\n",
    "        images.append({'features': image, 'labels': labels, 'id': row['ID']})\n",
    "    \n",
    "    return images, df.columns[1:]\n",
    "\n",
    "def calculate_ir_per_label(D, label_index):\n",
    "    label_counts = np.sum([instance['labels'] for instance in D], axis=0)\n",
    "    label_count = label_counts[label_index]\n",
    "    max_label_count = np.max(label_counts)\n",
    "    return max_label_count / label_count if label_count != 0 else float('inf')\n",
    "\n",
    "def calculate_mean_ir(D, labels):\n",
    "    irs = [calculate_ir_per_label(D, i) for i in range(len(labels))]\n",
    "    return np.mean(irs)\n",
    "\n",
    "def clone_sample(sample, new_id):\n",
    "    clone = {'features': sample['features'].copy(), 'labels': sample['labels'].copy(), 'id': new_id}\n",
    "    return clone\n",
    "\n",
    "def ml_ros(D, P):\n",
    "    print(f\"original dataset size: {len(D)}\")\n",
    "    samples_to_clone = int(len(D) * P / 100)\n",
    "    labels = range(len(D[0]['labels']))\n",
    "    mean_ir = calculate_mean_ir(D, labels)\n",
    "    \n",
    "    minority_bags = {label: [] for label in labels}\n",
    "    for label in labels:\n",
    "        ir_label = calculate_ir_per_label(D, label)\n",
    "        if ir_label > mean_ir:\n",
    "            for instance in D:\n",
    "                if instance['labels'][label] == 1:\n",
    "                    minority_bags[label].append(instance)\n",
    "    \n",
    "    new_id_counter = 0\n",
    "    while samples_to_clone > 0:\n",
    "        if not any(minority_bags.values()):\n",
    "            break\n",
    "        for label, bag in list(minority_bags.items()):\n",
    "            if not bag:\n",
    "                continue\n",
    "            sample_index = random.randint(0, len(bag) - 1)\n",
    "            sample = bag[sample_index]\n",
    "            clone = clone_sample(sample, f'DA_DA_{new_id_counter}')\n",
    "            D.append(clone)\n",
    "            new_id_counter += 1\n",
    "            samples_to_clone -= 1\n",
    "            if calculate_ir_per_label(D, label) <= mean_ir:\n",
    "                minority_bags.pop(label)\n",
    "            if samples_to_clone <= 0:\n",
    "                break\n",
    "    \n",
    "    print(f\"added {new_id_counter+1} samples\")\n",
    "    return D\n",
    "\n",
    "\n",
    "csv_path = '../data/fundus/MuReD/remedial_train_data.csv'\n",
    "image_dir = '../data/fundus/MuReD/images/images/'\n",
    "remedial_image_dir = '../data/fundus/MuReD/images/remedial/'\n",
    "dataset, labels = load_dataset(csv_path, image_dir, remedial_image_dir=remedial_image_dir)\n",
    "preprocessed_dataset = ml_ros(dataset, P=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID        aria_c_25_1aria_c_7_2aria_c_38_2aria_c_2_8aria...\n",
      "DR                                                      396\n",
      "NORMAL                                                  395\n",
      "MH                                                      135\n",
      "ODC                                                     211\n",
      "TSLN                                                    125\n",
      "ARMD                                                    126\n",
      "DN                                                      130\n",
      "MYA                                                      71\n",
      "BRVO                                                     63\n",
      "ODP                                                      62\n",
      "CRVO                                                     58\n",
      "CNV                                                      61\n",
      "RS                                                       58\n",
      "ODE                                                      56\n",
      "LS                                                       56\n",
      "CSR                                                      56\n",
      "HTR                                                      57\n",
      "ASR                                                      57\n",
      "CRS                                                      56\n",
      "OTHER                                                   209\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def save_preprocessed_dataset(D, labels, image_dir, output_csv_path):\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "\n",
    "    label_data = []\n",
    "    image_names = []\n",
    "    for instance in D:\n",
    "        if 'DA_' in instance['id']:\n",
    "            image_path = os.path.join(image_dir, f\"{instance['id']}.png\")\n",
    "            image = Image.fromarray(instance['features'])\n",
    "            image.save(image_path)\n",
    "        else:\n",
    "            possible_extensions = ['.tif', '.png']\n",
    "            for ext in possible_extensions:\n",
    "                temp_path = os.path.join(image_dir, f\"{instance['id']}{ext}\")\n",
    "                if os.path.exists(temp_path):\n",
    "                    image_path = temp_path\n",
    "                    break\n",
    "            \n",
    "        label_data.append(instance['labels'])\n",
    "        image_names.append(instance['id'])\n",
    "\n",
    "    label_df = pd.DataFrame(label_data, columns=labels)\n",
    "    label_df.insert(0, 'ID', image_names)\n",
    "    label_df.to_csv(os.path.join(output_csv_path, 'mlros_remedial_train_data.csv'), index=False)\n",
    "    \n",
    "    counts = label_df.sum(axis=0)\n",
    "    counts.to_dict()\n",
    "    print(counts)\n",
    "    \n",
    "    \n",
    "output_dir = '../data/fundus/MuReD/images/mlros_remedial/'\n",
    "output_csv_path = '../data/fundus/MuReD/'\n",
    "save_preprocessed_dataset(preprocessed_dataset, labels, output_dir, output_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilabel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
