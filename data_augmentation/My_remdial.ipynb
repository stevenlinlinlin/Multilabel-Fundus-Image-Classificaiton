{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import copy\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(csv_path, image_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    images = []\n",
    "    possible_extensions = ['.tif', '.png']\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        image_path = None\n",
    "        for ext in possible_extensions:\n",
    "            temp_path = os.path.join(image_dir, f\"{row['ID']}{ext}\")\n",
    "            if os.path.exists(temp_path):\n",
    "                image_path = temp_path\n",
    "                break\n",
    "        if image_path is None:\n",
    "            raise FileNotFoundError(f\"No image found for ID {row['ID']} with extensions {possible_extensions}\")\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        image = image.resize((384,384))  # resize the image to 384x384\n",
    "        \n",
    "        image = np.array(image)\n",
    "        labels = row.drop('ID').to_numpy()\n",
    "        images.append({'features': image, 'labels': labels, 'id': row['ID']})\n",
    "    \n",
    "    return images, df.columns[1:]\n",
    "\n",
    "\n",
    "csv_path = '../data/fundus/MuReD/train_data.csv'\n",
    "image_dir = '../data/fundus/MuReD/images/images/'\n",
    "dataset, labels = load_dataset(csv_path, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_sample(sample, new_id):\n",
    "    clone = {'features': sample['features'].copy(), 'labels': sample['labels'].copy(), 'id': new_id}\n",
    "    return clone\n",
    "\n",
    "def calculate_ir_lbl(D, label_index):\n",
    "    label_counts = np.sum([instance['labels'] for instance in D], axis=0)\n",
    "    label_count = label_counts[label_index]\n",
    "    max_label_count = np.max(label_counts)\n",
    "    return max_label_count / label_count if label_count != 0 else float('inf')\n",
    "\n",
    "def calculate_scu_mble_ins(instance, ir_lbl):\n",
    "    instance_labels = np.where(instance['labels'] == 1)[0]\n",
    "    k = len(instance_labels)\n",
    "    if k == 0:\n",
    "        return 0\n",
    "    product_ir_lbl = reduce(lambda x, y: x * y, (ir_lbl[label] for label in instance_labels), 1)\n",
    "    mean_ir_lbl = np.mean([ir_lbl[label] for label in instance_labels]) if instance_labels.size > 0 else 1\n",
    "    scumble_ins = 1 - (1 / mean_ir_lbl) * (product_ir_lbl ** (1 / k))\n",
    "    return scumble_ins\n",
    "\n",
    "def calculate_scu_mble(D, ir_lbl):\n",
    "    scumble_ins = [calculate_scu_mble_ins(instance, ir_lbl) for instance in D]\n",
    "    scumble = np.mean(scumble_ins)\n",
    "    return scumble, scumble_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour(X):\n",
    "    nbs=NearestNeighbors(n_neighbors=3,metric='euclidean',algorithm='kd_tree').fit(X)\n",
    "    _,indices= nbs.kneighbors(X)\n",
    "    return indices\n",
    "\n",
    "def smote(ref, nn_indices, imbalance_images):\n",
    "    neighbour = random.choice(nn_indices[ref,1:])\n",
    "    ratio = random.random()\n",
    "    gap = imbalance_images[ref,:] - imbalance_images[neighbour,:]\n",
    "    new_x = np.array(imbalance_images[ref,:] + ratio * gap)\n",
    "    return new_x.reshape(384,384,3).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir_lbl: [1.0, 1.0025316455696203, 2.933333333333333, 1.8767772511848342, 3.168, 3.142857142857143, 3.046153846153846, 5.577464788732394, 6.285714285714286, 7.92, 9.0, 8.25, 8.425531914893616, 8.608695652173912, 10.702702702702704, 13.655172413793103, 14.142857142857142, 15.23076923076923, 16.5, 1.894736842105263]\n",
      "ir_mean: 7.1181649096420205\n",
      "scumble: 0.028608447515591443\n",
      "imbalance index: 92\n",
      "added 706 samples\n",
      "Counter({14: 245, 9: 161, 16: 77, 17: 61, 18: 51, 11: 40, 13: 30, 10: 24, 15: 22, 12: 8})\n"
     ]
    }
   ],
   "source": [
    "def my_remedial(ori_D, P, threshold=0.5):\n",
    "    D = copy.deepcopy(ori_D)\n",
    "    samples_to_clone = int(len(D) * P)\n",
    "    # print(f\"cloning {samples_to_clone} samples\")\n",
    "    \n",
    "    # Calculate imbalance levels\n",
    "    ir_lbl = [calculate_ir_lbl(D, i) for i in range(len(labels))]\n",
    "    ir_mean = np.mean(ir_lbl)\n",
    "    print(f\"ir_lbl: {ir_lbl}\")\n",
    "    print(f\"ir_mean: {ir_mean}\")\n",
    "    # Calculate SCUMBLE\n",
    "    scumble, scumble_ins = calculate_scu_mble(D, ir_lbl)\n",
    "    print(f\"scumble: {scumble}\")\n",
    "    \n",
    "    imbalance_index = []\n",
    "    # imbalance_images = []\n",
    "    for i in range(len(D)):\n",
    "        if scumble_ins[i] > scumble:\n",
    "            imbalance_index.append(i)\n",
    "            # imbalance_images.append(D[i]['features'].flatten())\n",
    "    \n",
    "    imbalance_images = np.array(imbalance_images)\n",
    "    # indices = nearest_neighbour(imbalance_images)\n",
    "    \n",
    "    print(f\"imbalance index: {len(imbalance_index)}\")\n",
    "    new_id_counter = 0\n",
    "    new_instances = []\n",
    "    new_labels_counter = Counter()\n",
    "    while samples_to_clone > 0:\n",
    "        samples = random.sample(imbalance_index, min(samples_to_clone, len(imbalance_index)))\n",
    "        for sample_index in samples:\n",
    "            # smote\n",
    "            # ref = imbalance_index.index(sample_index)\n",
    "            # new_x = smote(ref, indices, imbalance_images)\n",
    "            \n",
    "            instance = D[sample_index]\n",
    "            clone_instance = copy.deepcopy(instance)\n",
    "            clone_instance['id'] = f\"DA_{new_id_counter}\"\n",
    "            # clone_instance['features'] = new_x\n",
    "            \n",
    "            # Add minority labels\n",
    "            clone_instance['labels'][[label for label in range(len(labels)) if ir_lbl[label] <= ir_mean]] = 0\n",
    "            \n",
    "            new_instances.append(clone_instance)\n",
    "            new_labels_counter.update(np.where(clone_instance['labels'] == 1)[0])\n",
    "            new_id_counter += 1\n",
    "            samples_to_clone -= 1\n",
    "            if samples_to_clone <= 0:\n",
    "                break\n",
    "            \n",
    "    D.extend(new_instances)\n",
    "    print(f\"added {new_id_counter+1} samples\")\n",
    "    print(new_labels_counter)\n",
    "    return D\n",
    "\n",
    "\n",
    "preprocessed_dataset = my_remedial(dataset, P=0.4)#, threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID        aria_c_25_1aria_c_7_2aria_c_38_2aria_c_2_8aria...\n",
      "DR                                                      396\n",
      "NORMAL                                                  395\n",
      "MH                                                      135\n",
      "ODC                                                     211\n",
      "TSLN                                                    125\n",
      "ARMD                                                    126\n",
      "DN                                                      130\n",
      "MYA                                                      71\n",
      "BRVO                                                     63\n",
      "ODP                                                     211\n",
      "CRVO                                                     68\n",
      "CNV                                                      88\n",
      "RS                                                       55\n",
      "ODE                                                      76\n",
      "LS                                                      282\n",
      "CSR                                                      51\n",
      "HTR                                                     105\n",
      "ASR                                                      87\n",
      "CRS                                                      75\n",
      "OTHER                                                   209\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def save_preprocessed_dataset(D, labels, image_dir, output_csv_path):\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "\n",
    "    label_data = []\n",
    "    image_names = []\n",
    "    for instance in D:\n",
    "        if 'DA_' in instance['id']:\n",
    "            image_path = os.path.join(image_dir, f\"{instance['id']}.png\")\n",
    "            image = Image.fromarray(instance['features'])\n",
    "            image.save(image_path)\n",
    "        else:\n",
    "            possible_extensions = ['.tif', '.png']\n",
    "            for ext in possible_extensions:\n",
    "                temp_path = os.path.join(image_dir, f\"{instance['id']}{ext}\")\n",
    "                if os.path.exists(temp_path):\n",
    "                    image_path = temp_path\n",
    "                    break\n",
    "            \n",
    "        label_data.append(instance['labels'])\n",
    "        image_names.append(instance['id'])\n",
    "\n",
    "    label_df = pd.DataFrame(label_data, columns=labels)\n",
    "    label_df.insert(0, 'ID', image_names)\n",
    "    label_df.to_csv(os.path.join(output_csv_path, 'my_remedial_smote384_train_data.csv'), index=False)\n",
    "    \n",
    "    counts = label_df.sum(axis=0)\n",
    "    counts.to_dict()\n",
    "    print(counts)\n",
    "\n",
    "\n",
    "output_dir = '../data/fundus/MuReD/images/my_remedial_smote384/'\n",
    "output_csv_path = '../data/fundus/MuReD/'\n",
    "save_preprocessed_dataset(preprocessed_dataset, labels, output_dir, output_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilabel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
